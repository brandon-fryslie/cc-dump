# EVALUATION: HAR Recording and Replay
Generated: 2026-02-03T12:00:00
Source: Manual analysis of codebase for cc-dump-i17

## Current State

### What Exists

1. **Event Pipeline (proxy.py -> router.py -> subscribers)**
   - proxy.py emits raw event tuples: `("request", body)`, `("request_headers", headers)`, `("response_headers", status, headers)`, `("response_event", type, data)`, `("response_done",)`, `("error", code, reason)`, `("proxy_error", msg)`, `("log", command, path, status)`
   - router.py fans out to QueueSubscriber (TUI) and DirectSubscriber (SQLite)
   - Events are Python tuples, not serialized

2. **SQLite Persistence (store.py)**
   - SQLiteWriter accumulates request/response pairs into complete "turns"
   - Stores blobified request_json and response_json (with large strings extracted to blobs table)
   - Loses raw HTTP headers (neither request nor response headers are persisted)
   - Loses individual SSE event boundaries (response_events are accumulated, not stored individually)
   - Loses event ordering/timing information
   - No way to replay: data is post-processed (blobified, aggregated) before storage

3. **TUI Event Consumption (event_handlers.py -> formatting.py)**
   - event_handlers.py receives raw event tuples and calls formatting functions
   - formatting.py converts API JSON into FormattedBlock IR
   - The TUI pipeline processes events one-at-a-time in sequence
   - Content tracking state (system prompt hashing, position tracking) is accumulated across events

4. **CLI Bootstrap (cli.py)**
   - Creates event_q, router, display_sub, SQLiteWriter, and CcDumpApp
   - No concept of "replay mode" or loading from saved state
   - Content tracking state dict is initialized fresh each session

### What Does NOT Exist

1. No raw HTTP recording capability
2. No replay/restore mechanism
3. No unified data source that serves both live and replay modes
4. No serialization of the event stream (only post-processed SQLite storage)
5. No --replay CLI flag or equivalent
6. No event serializer/deserializer

### Architectural Gap

The ticket's critical requirement is: **"Loading from serialization must become the DEFAULT/ONLY code path"** with **"Zero divergence between live display and restore session"**.

Currently there are TWO separate paths:
- **Live path**: proxy.py -> event tuples -> router -> {TUI subscriber, SQLite subscriber}
- **DB path**: SQLite stores aggregated turns (NOT replayable event streams)

The target architecture with HAR:
- **Live path**: proxy.py -> streaming events -> router -> {TUI subscriber, SQLite subscriber, HAR recorder}
  - HAR recorder accumulates events in memory, writes complete messages to HAR on completion
  - Critical: HAR recording is a THIRD subscriber, parallel to TUI (preserves streaming UX)
- **Replay path**: Load HAR -> convert complete messages to synthetic events -> push all to queue -> router -> {TUI subscriber, SQLite subscriber}
  - Batch processing: all events pushed at once, no streaming simulation
  - Router and formatting code process events identically (same event tuples, same pipeline)

**Divergence points:**
1. Live: real SSE events arrive incrementally. Replay: synthetic events pushed in batch.
2. Live: TUI updates as tokens arrive. Replay: TUI processes all events in tight loop, then displays.

**Convergence points:**
1. Event tuple format is identical (synthetic events match live format exactly)
2. Router, formatting, rendering code is completely shared (one pipeline)
3. Final application state is identical (same FormattedBlocks, same widgets, same display)

### Format Analysis

**HAR (HTTP Archive) format - CHOSEN APPROACH:**
- HAR captures HTTP request/response pairs (W3C standard format)
- Well-supported by Chrome DevTools, browser dev panels, various analysis tools
- **Key architectural decision**: Store SYNTHETIC non-streaming responses
  - Live: SSE stream received → accumulate events → reconstruct complete message → save to HAR
  - HAR contains: `"stream": false` requests and complete JSON message responses
  - Replay: load complete messages → convert to synthetic events → process through pipeline
- **Trade-off**: HAR is not wire-faithful (shows synthetic responses, not actual SSE streams)
- **Benefit**: Standard format, tooling compatibility, simpler replay (no SSE parsing needed)

**Why not JSONL event log:**
- Would preserve exact SSE event boundaries and timing
- Better for wire-faithful recording
- But: custom format, no standard tooling, harder to inspect/debug
- And: we don't need streaming during replay (negative priority per user requirement)

**Final decision**: HAR format with synthetic complete messages. Accepts wire-unfaithfulness in exchange for standard format and simpler replay architecture.

## Quantitative Metrics

- Event types to handle: 8 (request, request_headers, response_headers, response_event, response_done, error, proxy_error, log)
- New modules: 2 (har_recorder.py, har_replayer.py)
- Modules requiring changes: 1 (cli.py - add --record/--no-record flags and --replay flag)
- Modules with zero changes needed: formatting.py, rendering.py, widget_factory.py, event_handlers.py, router.py (completely unaware of HAR)
- Test coverage needed: HAR generation (SSE → complete message), message-to-event conversion (complete message → synthetic SSE), CLI integration, state restoration verification
